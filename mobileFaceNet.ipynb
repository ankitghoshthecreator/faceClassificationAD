{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68723dcb-8c2b-4d71-ade3-5c41475f5548",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f4b02-83d6-4ed2-a09d-d29a030c510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python numpy pandas facenet-pytorch torch torchvision onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b4eac2-484a-4994-921a-d907c33ef939",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf92a47-587b-4e44-9a07-fcab7f3a76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8463f9c6-1ba4-4352-9764-abfde4bfcba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.prelu = nn.PReLU(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.prelu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e0ef5b-3325-4944-92e7-4cfcff368b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthWise(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.prelu = nn.PReLU(in_channels)\n",
    "        self.pw = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelu(self.bn1(self.dw(x)))\n",
    "        x = self.bn2(self.pw(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd6b0fc-0f48-4343-9d48-65243432495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.dw = DepthWise(out_channels, out_channels, stride)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.use_shortcut = (stride == 1 and in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dw(self.conv1(x))\n",
    "        if self.use_shortcut:\n",
    "            out = out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebefd640-0ba9-4070-a22c-bfd346129123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileFaceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = ConvBlock(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.layer2 = DepthWise(64, 64, stride=1)\n",
    "        self.layer3 = self._make_layer(64, 64, stride=2, num_blocks=4)\n",
    "        self.layer4 = self._make_layer(64, 128, stride=2, num_blocks=6)\n",
    "        self.layer5 = self._make_layer(128, 128, stride=2, num_blocks=2)\n",
    "        self.conv6 = ConvBlock(128, 512, kernel_size=1, stride=1, padding=0)\n",
    "        self.dw7 = nn.Conv2d(512, 512, kernel_size=7, groups=512, bias=False)  # global depthwise\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(512, 128)\n",
    "        self.bn8 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, stride, num_blocks):\n",
    "        layers = [Bottleneck(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(Bottleneck(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.dw7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.bn8(x)\n",
    "        return F.normalize(x, p=2, dim=1)  # L2 normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c28b565-71f1-485d-addd-95f93b835078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output embedding shape: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "model = MobileFaceNet()\n",
    "\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, 112, 112)  # 1 RGB face image\n",
    "embedding = model(dummy_input)\n",
    "print(\"Output embedding shape:\", embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea8959-8c11-40b4-9d11-d9cbddc59b44",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e8d64-0ac7-4cf1-9d21-f36ef07127f2",
   "metadata": {},
   "source": [
    "### mode 1: data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf01a59-f2f4-4a4a-afe8-fd8d0ce2464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the person's name:  ankit\n",
      "Choose input method:\n",
      "1 - Use webcam\n",
      "2 - Upload image\n",
      "Your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'c' to capture, 'q' to quit.\n",
      "[1] Saved embedding for ankit\n",
      "[2] Saved embedding for ankit\n",
      "[3] Saved embedding for ankit\n",
      "[4] Saved embedding for ankit\n",
      "[5] Saved embedding for ankit\n",
      "[6] Saved embedding for ankit\n",
      "[7] Saved embedding for ankit\n",
      "[8] Saved embedding for ankit\n",
      "[9] Saved embedding for ankit\n",
      "[10] Saved embedding for ankit\n",
      "[11] Saved embedding for ankit\n",
      "[12] Saved embedding for ankit\n",
      "[13] Saved embedding for ankit\n",
      "[14] Saved embedding for ankit\n",
      "[15] Saved embedding for ankit\n",
      "[16] Saved embedding for ankit\n",
      "[17] Saved embedding for ankit\n",
      "[18] Saved embedding for ankit\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import threading\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchvision import transforms\n",
    "from tkinter import Tk, filedialog\n",
    "from PIL import Image\n",
    "\n",
    "# Setup\n",
    "mtcnn = MTCNN(keep_all=True)\n",
    "model = MobileFaceNet()\n",
    "model.eval()\n",
    "\n",
    "CSV_FILE = \"face_dataset.csv\"\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Input: name\n",
    "name = input(\"Enter the person's name: \")\n",
    "\n",
    "# Input: mode\n",
    "mode = input(\"Choose input method:\\n1 - Use webcam\\n2 - Upload image\\nYour choice: \")\n",
    "\n",
    "if mode.strip() == \"1\":\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"Press 'c' to capture, 'q' to quit.\")\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box in boxes[:1]:  # only first face\n",
    "                x1, y1, x2, y2 = [int(v) for v in box]\n",
    "                face = img[y1:y2, x1:x2]\n",
    "                if face.size == 0:\n",
    "                    continue\n",
    "                face = cv2.resize(face, (112, 112))\n",
    "                face_bgr = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imshow(\"Face\", face_bgr)\n",
    "\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('c') and boxes is not None:\n",
    "            face_tensor = to_tensor(face).unsqueeze(0).float()\n",
    "            with torch.no_grad():\n",
    "                embedding = model(face_tensor).squeeze().numpy()\n",
    "            row = [name] + embedding.tolist()\n",
    "            pd.DataFrame([row]).to_csv(CSV_FILE, mode='a', index=False, header=False)\n",
    "            print(f\"[{count + 1}] Saved embedding for {name}\")\n",
    "            count += 1\n",
    "\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ==== MODE 2: UPLOAD ====\n",
    "elif mode.strip() == \"2\":\n",
    "    file_path = input(\"Paste the full path of the image file (jpg/png): \").strip()\n",
    "\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        try:\n",
    "            img = Image.open(file_path).convert(\"RGB\")\n",
    "            img_np = np.array(img)\n",
    "\n",
    "            boxes, _ = mtcnn.detect(img_np)\n",
    "\n",
    "            if boxes is not None:\n",
    "                x1, y1, x2, y2 = [int(v) for v in boxes[0]]\n",
    "                face = img_np[y1:y2, x1:x2]\n",
    "                if face.size != 0:\n",
    "                    face = cv2.resize(face, (112, 112))\n",
    "                    face_tensor = to_tensor(face).unsqueeze(0).float()\n",
    "                    with torch.no_grad():\n",
    "                        embedding = model(face_tensor).squeeze().numpy()\n",
    "                    row = [name] + embedding.tolist()\n",
    "                    pd.DataFrame([row]).to_csv(CSV_FILE, mode='a', index=False, header=False)\n",
    "                    print(f\"✅ Saved embedding for {name} from uploaded image.\")\n",
    "                else:\n",
    "                    print(\"❌ Face crop is empty.\")\n",
    "            else:\n",
    "                print(\"❌ No face detected in image.\")\n",
    "        except Exception as e:\n",
    "            print(\"❌ Error processing image:\", e)\n",
    "    else:\n",
    "        print(\"❌ File not found. Please check the path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3efae24-83ff-4ffe-99b7-f27f46678d5e",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65fef229-bb38-4d2a-bfde-6e8ab176ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', 'dataCollection.ipynb', 'mobileFaceNet.ipynb', '.ipynb_checkpoints', 'concatCsv.ipynb', 'mobilefacenet.pth', 'face_dataset.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())  # Shows files in current working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c87ff14-265a-48d3-a96b-88425bbda011",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mobilefacenet.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15bbd3d7-0bd0-4ce8-a1ff-16426a2bb891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models: ['mobilefacenet.pth']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Saved models:\", [f for f in os.listdir() if f.endswith('.pth')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2350c82-8520-404c-a873-2933b8ad82ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ====== MobileFaceNet Model (Inline) ======\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.prelu = nn.PReLU(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.prelu(self.bn(self.conv(x)))\n",
    "\n",
    "class DepthWise(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.prelu = nn.PReLU(in_channels)\n",
    "        self.pw = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelu(self.bn1(self.dw(x)))\n",
    "        x = self.bn2(self.pw(x))\n",
    "        return x\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.dw = DepthWise(out_channels, out_channels, stride)\n",
    "        self.use_shortcut = (stride == 1 and in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dw(self.conv1(x))\n",
    "        if self.use_shortcut:\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "class MobileFaceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = ConvBlock(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.layer2 = DepthWise(64, 64, stride=1)\n",
    "        self.layer3 = self._make_layer(64, 64, stride=2, num_blocks=4)\n",
    "        self.layer4 = self._make_layer(64, 128, stride=2, num_blocks=6)\n",
    "        self.layer5 = self._make_layer(128, 128, stride=2, num_blocks=2)\n",
    "        self.conv6 = ConvBlock(128, 512, kernel_size=1, stride=1, padding=0)\n",
    "        self.dw7 = nn.Conv2d(512, 512, kernel_size=7, groups=512, bias=False)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(512, 128)\n",
    "        self.bn8 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, stride, num_blocks):\n",
    "        layers = [Bottleneck(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(Bottleneck(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.dw7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.bn8(x)\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "# ====== Load face_dataset.csv ======\n",
    "df = pd.read_csv(\"vertically_combined_face_dataset.csv\")\n",
    "names = df.iloc[:, 0].values\n",
    "embeddings = df.iloc[:, 1:].values.astype('float32')\n",
    "\n",
    "# ====== Encode labels ======\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(names)\n",
    "\n",
    "# ====== Train SVM classifier ======\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "clf.fit(embeddings, labels)\n",
    "\n",
    "# ====== Setup MTCNN & MobileFaceNet ======\n",
    "model = MobileFaceNet()\n",
    "model.eval()\n",
    "to_tensor = transforms.ToTensor()\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ====== Start Webcam for Recognition ======\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    boxes, _ = mtcnn.detect(img)\n",
    "\n",
    "    if boxes is not None:\n",
    "        x1, y1, x2, y2 = [int(v) for v in boxes[0]]\n",
    "        face = img[y1:y2, x1:x2]\n",
    "        if face.size != 0:\n",
    "            face = cv2.resize(face, (112, 112))\n",
    "            face_tensor = to_tensor(face).unsqueeze(0).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                emb = model(face_tensor).numpy()\n",
    "\n",
    "            # Predict using SVM\n",
    "            pred = clf.predict(emb)[0]\n",
    "            proba = clf.predict_proba(emb)[0].max()\n",
    "            name = label_encoder.inverse_transform([pred])[0]\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{name} ({proba:.2f})\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Face Recognition (SVM)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94876a79-2d78-40eb-b3a7-c81032c3a516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
