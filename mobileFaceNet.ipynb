{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68723dcb-8c2b-4d71-ade3-5c41475f5548",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1f4b02-83d6-4ed2-a09d-d29a030c510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python numpy pandas facenet-pytorch torch torchvision onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b4eac2-484a-4994-921a-d907c33ef939",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf92a47-587b-4e44-9a07-fcab7f3a76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717b464c-f71d-41ef-bd5c-fbaa8b4c4609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.prelu = nn.PReLU(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.prelu(self.bn(self.conv(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d82bddd2-4661-4848-83b9-ba82c423367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthWise(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.prelu = nn.PReLU(in_channels)\n",
    "        self.pw = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelu(self.bn1(self.dw(x)))\n",
    "        x = self.bn2(self.pw(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa498b9-70fe-4a2e-843e-60d5db0fb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.dw = DepthWise(out_channels, out_channels, stride)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.use_shortcut = (stride == 1 and in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dw(self.conv1(x))\n",
    "        if self.use_shortcut:\n",
    "            out = out + x\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebefd640-0ba9-4070-a22c-bfd346129123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileFaceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = ConvBlock(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.layer2 = DepthWise(64, 64, stride=1)\n",
    "        self.layer3 = self._make_layer(64, 64, stride=2, num_blocks=4)\n",
    "        self.layer4 = self._make_layer(64, 128, stride=2, num_blocks=6)\n",
    "        self.layer5 = self._make_layer(128, 128, stride=2, num_blocks=2)\n",
    "        self.conv6 = ConvBlock(128, 512, kernel_size=1, stride=1, padding=0)\n",
    "        self.dw7 = nn.Conv2d(512, 512, kernel_size=7, groups=512, bias=False)  # global depthwise\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(512, 128)\n",
    "        self.bn8 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, stride, num_blocks):\n",
    "        layers = [Bottleneck(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(Bottleneck(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.dw7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.bn8(x)\n",
    "        return F.normalize(x, p=2, dim=1)  # L2 normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c28b565-71f1-485d-addd-95f93b835078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output embedding shape: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "model = MobileFaceNet()\n",
    "\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, 112, 112)  # 1 RGB face image\n",
    "embedding = model(dummy_input)\n",
    "print(\"Output embedding shape:\", embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea8959-8c11-40b4-9d11-d9cbddc59b44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0657e036-7f9c-49ab-8e24-df132c00d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from facenet_pytorch import MTCNN\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "mtcnn = MTCNN(keep_all=False)  # Only 1 face at a time\n",
    "\n",
    "CSV_FILE = 'face_dataset1.csv'\n",
    "\n",
    "# Create CSV if not exists\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    df = pd.DataFrame(columns=[\"name\"] + [f\"emb_{i}\" for i in range(128)])\n",
    "    df.to_csv(CSV_FILE, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569ac635-447a-4280-a163-7f1f053461dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the person's name:  ag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Sample captured for ag\n",
      "[2] Sample captured for ag\n",
      "[3] Sample captured for ag\n",
      "[4] Sample captured for ag\n",
      "[5] Sample captured for ag\n",
      "[6] Sample captured for ag\n",
      "[7] Sample captured for ag\n",
      "[8] Sample captured for ag\n",
      "[9] Sample captured for ag\n",
      "[10] Sample captured for ag\n"
     ]
    }
   ],
   "source": [
    "name = input(\"Enter the person's name: \")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "count = 0\n",
    "MAX_SAMPLES = 10\n",
    "\n",
    "while count < MAX_SAMPLES:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    boxes, probs = mtcnn.detect(img)\n",
    "\n",
    "    if boxes is not None:\n",
    "        box = boxes[0]  # Only first face\n",
    "        x1, y1, x2, y2 = [int(v) for v in box]\n",
    "        face = img[y1:y2, x1:x2]\n",
    "        if face.size == 0:\n",
    "            continue\n",
    "        face = cv2.resize(face, (112, 112))\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.imshow(\"Face\", cv2.cvtColor(face, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key & 0xFF == ord('c') and boxes is not None:\n",
    "        face_tensor = transforms.ToTensor()(face).unsqueeze(0) \n",
    "        face_tensor = face_tensor.float()  \n",
    "        with torch.no_grad():\n",
    "            embedding = model(face_tensor).squeeze().numpy()\n",
    "\n",
    "\n",
    "        \n",
    "        row = [name] + embedding.tolist()\n",
    "        pd.DataFrame([row]).to_csv(CSV_FILE, mode='a', index=False, header=False)\n",
    "        count += 1\n",
    "        print(f\"[{count}] Sample captured for {name}\")\n",
    "\n",
    "    elif key & 0xFF == ord('q'):\n",
    "        print(\"Exit requested.\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fef229-bb38-4d2a-bfde-6e8ab176ab2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
